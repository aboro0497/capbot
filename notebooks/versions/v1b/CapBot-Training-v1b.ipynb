{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1r/xbh8qf_s1cb25q4t_1_v4bzh0000gn/T/ipykernel_88979/4211160408.py:20: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Threshold  Total Predictions  Correct Predictions  Accuracy\n",
      "0    >= 0.6              15260                11565    0.7579\n",
      "1    >= 0.7               8345                 6898    0.8266\n",
      "2    >= 0.8               4140                 3658    0.8836\n",
      "3    >= 0.9               1930                 1792    0.9285\n",
      "\n",
      "ðŸ“„ Report saved to: /Users/boroni_4/Documents/CapBot/capbot/notebooks/versions/v1b/CapBot_v1b_Confidence_Report_20250516.csv\n",
      "ðŸ“Š XLSX saved to: /Users/boroni_4/Documents/CapBot/capbot/notebooks/versions/v1b/CapBot_v1b_Confidence_Report_20250516.xlsx\n",
      "ðŸ“„ README saved to: /Users/boroni_4/Documents/CapBot/capbot/notebooks/versions/v1b/CapBot_v1b_README.md\n"
     ]
    }
   ],
   "source": [
    "# === CapBot v1b â€“ Confidence Accuracy Analysis ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# === Setup ===\n",
    "today = datetime.today().strftime(\"%Y%m%d\")\n",
    "version = \"v1b\"\n",
    "notebook_dir = Path(__file__).parent if \"__file__\" in globals() else Path().resolve()\n",
    "version_dir = notebook_dir\n",
    "report_path = version_dir / f\"CapBot_{version}_Confidence_Report_{today}.csv\"\n",
    "readme_path = version_dir / f\"CapBot_{version}_README.md\"\n",
    "xlsx_path = version_dir / f\"CapBot_{version}_Confidence_Report_{today}.xlsx\"\n",
    "\n",
    "# === Load Data ===\n",
    "file_path = version_dir / \"../../../data/historical/processed/matches_2015_2025_combined_balanced.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "numeric_cols = ['rank_A', 'rank_B', 'pts_A', 'pts_B', 'odds_A', 'odds_B']\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "df = df.dropna(subset=numeric_cols + ['winner_code'])\n",
    "\n",
    "# === Train on Full Dataset ===\n",
    "X = df[numeric_cols]\n",
    "y = df['winner_code']\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# === Predict on Full Dataset ===\n",
    "preds_proba = model.predict_proba(X)[:, 1]\n",
    "preds = (preds_proba > 0.5).astype(int)\n",
    "df['pred_proba'] = preds_proba\n",
    "df['predicted'] = preds\n",
    "df['correct'] = (preds == y).astype(int)\n",
    "\n",
    "# === Analyze Confidence Thresholds ===\n",
    "thresholds = [0.6, 0.7, 0.8, 0.9]\n",
    "rows = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    confident = df[df['pred_proba'] >= threshold]\n",
    "    total = len(confident)\n",
    "    correct = confident['correct'].sum()\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    rows.append({\n",
    "        \"Threshold\": f\">= {threshold}\",\n",
    "        \"Total Predictions\": total,\n",
    "        \"Correct Predictions\": correct,\n",
    "        \"Accuracy\": round(accuracy, 4)\n",
    "    })\n",
    "\n",
    "# === Output Results ===\n",
    "summary_df = pd.DataFrame(rows)\n",
    "summary_df.to_csv(report_path, index=False)\n",
    "\n",
    "# Save to XLSX with total match summary\n",
    "with pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\") as writer:\n",
    "    summary_df.to_excel(writer, sheet_name=\"Confidence Summary\", index=False)\n",
    "    workbook  = writer.book\n",
    "    worksheet = writer.sheets[\"Confidence Summary\"]\n",
    "    worksheet.write(len(summary_df) + 2, 0, \"Total Matches\")\n",
    "    worksheet.write(len(summary_df) + 2, 1, len(df))\n",
    "    for i, row in enumerate(rows):\n",
    "        threshold_val = float(row[\"Threshold\"].split()[-1])\n",
    "        count = row[\"Total Predictions\"]\n",
    "        acc = row[\"Accuracy\"]\n",
    "        worksheet.write(len(summary_df) + 3 + i, 0, f\"â‰¥ {threshold_val}\")\n",
    "        worksheet.write(len(summary_df) + 3 + i, 1, f\"{count} matches\")\n",
    "        worksheet.write(len(summary_df) + 3 + i, 2, f\"{acc:.2%} accuracy\")\n",
    "\n",
    "print(summary_df)\n",
    "print(f\"\\nðŸ“„ Report saved to: {report_path.resolve()}\")\n",
    "print(f\"ðŸ“Š XLSX saved to: {xlsx_path.resolve()}\")\n",
    "\n",
    "# === Generate README ===\n",
    "total_matches = len(df)\n",
    "summary_lines = [f\"Out of **{total_matches} total matches**:\"]\n",
    "\n",
    "for threshold in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    confident = df[df['pred_proba'] >= threshold]\n",
    "    total = len(confident)\n",
    "    correct = confident['correct'].sum()\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    summary_lines.append(f\"- **{total} matches** had `pred_proba â‰¥ {threshold}` â†’ Accuracy: **{accuracy:.2%}**\")\n",
    "\n",
    "confidence_summary = \"\\n\".join(summary_lines)\n",
    "\n",
    "readme = f\"\"\"\\\n",
    "# ðŸ“„ CapBot {version.upper()} â€“ Confidence Calibration Report\n",
    "\n",
    "**Date:** {today}  \n",
    "**Report File:** `{report_path.name}`\n",
    "**Excel Report:** `{xlsx_path.name}`\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "To evaluate the baseline model's confidence calibration by analyzing accuracy at various prediction probability thresholds using the **entire historical dataset** (~{total_matches} matches).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Methodology\n",
    "- Trained a logistic regression model using all historical matches\n",
    "- Predicted win probabilities (`pred_proba`) for each match\n",
    "- Measured how accurate the model is when its confidence exceeds specific thresholds\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Confidence Threshold Summary\n",
    "{confidence_summary}\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ Raw Table View\n",
    "{summary_df.to_markdown(index=False)}\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Insights\n",
    "- Confidence thresholds correlate positively with accuracy\n",
    "- Higher confidence yields fewer predictions, but better reliability\n",
    "- This lays the groundwork for smarter filtering and EV-based strategies\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(f\"ðŸ“„ README saved to: {readme_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
