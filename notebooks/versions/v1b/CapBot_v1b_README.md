# ðŸ“„ CapBot V1B â€“ Confidence Calibration Report

**Date:** 20250516  
**Report File:** `CapBot_v1b_Confidence_Report_20250516.csv`
**Excel Report:** `CapBot_v1b_Confidence_Report_20250516.xlsx`

---

## ðŸŽ¯ Objective
To evaluate the baseline model's confidence calibration by analyzing accuracy at various prediction probability thresholds using the **entire historical dataset** (~50279 matches).

---

## ðŸ§  Methodology
- Trained a logistic regression model using all historical matches
- Predicted win probabilities (`pred_proba`) for each match
- Measured how accurate the model is when its confidence exceeds specific thresholds

---

## ðŸ“Š Confidence Threshold Summary
Out of **50279 total matches**:
- **24981 matches** had `pred_proba â‰¥ 0.5` â†’ Accuracy: **68.02%**
- **15260 matches** had `pred_proba â‰¥ 0.6` â†’ Accuracy: **75.79%**
- **8345 matches** had `pred_proba â‰¥ 0.7` â†’ Accuracy: **82.66%**
- **4140 matches** had `pred_proba â‰¥ 0.8` â†’ Accuracy: **88.36%**
- **1930 matches** had `pred_proba â‰¥ 0.9` â†’ Accuracy: **92.85%**

---

## ðŸ“ˆ Raw Table View
| Threshold   |   Total Predictions |   Correct Predictions |   Accuracy |
|:------------|--------------------:|----------------------:|-----------:|
| >= 0.6      |               15260 |                 11565 |     0.7579 |
| >= 0.7      |                8345 |                  6898 |     0.8266 |
| >= 0.8      |                4140 |                  3658 |     0.8836 |
| >= 0.9      |                1930 |                  1792 |     0.9285 |

---

## âœ… Insights
- Confidence thresholds correlate positively with accuracy
- Higher confidence yields fewer predictions, but better reliability
- This lays the groundwork for smarter filtering and EV-based strategies

---
